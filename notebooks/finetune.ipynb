{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.backends\n",
    "import torch.backends.cudnn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import SamProcessor\n",
    "from transformers import SamModel\n",
    "from losses import get_critn\n",
    "from utils import postprocess_masks, set_random_seed, mask_iou, DSC\n",
    "import random\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def erode_mask(mask: np.ndarray, kernel_size: int = 3) -> np.ndarray:\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    mask = cv2.erode(mask, kernel, iterations=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def dilate_mask(mask: np.ndarray,\n",
    "                kernel_size: int = 3,\n",
    "                iterations: int = 1) -> np.ndarray:\n",
    "    kernel: np.ndarray = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    mask = cv2.dilate(mask, kernel, iterations=iterations)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_point_prompt_bymask(mask: np.ndarray) -> Dict[str, List]:\n",
    "    \"\"\"\n",
    "    input_points: [nb_images, nb_predictions, nb_points_per_mask, 2]\n",
    "    \"\"\"\n",
    "    nb = 100\n",
    "    mask = erode_mask(mask)\n",
    "    bg_points = np.where(mask == 0)\n",
    "    bg_points = [list(i) for i in bg_points]\n",
    "    bg_points = list(zip(bg_points[1], bg_points[0]))\n",
    "    assert len(bg_points) > 0\n",
    "    indices = np.random.choice(\n",
    "        np.arange(len(bg_points)),\n",
    "        size=nb, replace=False) \\\n",
    "        if len(bg_points) >= nb else \\\n",
    "        np.random.choice(\n",
    "            np.arange(len(bg_points)),\n",
    "            size=nb, replace=True)\n",
    "    bg_points = np.array(bg_points)[indices]\n",
    "    bg_label = np.zeros(nb,)\n",
    "\n",
    "    fg_points = np.where(mask != 0)\n",
    "    fg_points = [list(i) for i in fg_points]\n",
    "    fg_points = list(zip(fg_points[1], fg_points[0]))\n",
    "    if len(fg_points) > 0:\n",
    "        indices = np.random.choice(\n",
    "            np.arange(len(fg_points)),\n",
    "            size=nb, replace=False) \\\n",
    "            if len(fg_points) >= nb else \\\n",
    "            np.random.choice(\n",
    "                np.arange(len(fg_points)),\n",
    "                size=nb, replace=True)\n",
    "        fg_points = np.array(fg_points)[indices]\n",
    "        fg_label = np.ones(nb,)\n",
    "        pmt_points = [[np.vstack([fg_points, bg_points]).tolist()]]\n",
    "        pmt_labels = [[np.hstack([fg_label, bg_label]).tolist()]]\n",
    "    else:\n",
    "        fg_points = []\n",
    "        fg_label = []\n",
    "        pmt_points = [[np.vstack([bg_points, bg_points]).tolist()]]\n",
    "        pmt_labels = [[np.hstack([bg_label, bg_label]).tolist()]]\n",
    "    # pmt_points = np.vstack([fg_points, bg_points])\n",
    "    # pmt_labels = np.hstack([fg_label, bg_label])\n",
    "    return {'input_points': pmt_points,\n",
    "            'input_labels': pmt_labels}\n",
    "\n",
    "\n",
    "def calc_metrics(pred_masks: torch.Tensor,\n",
    "                 gt_mask: torch.Tensor, batch) \\\n",
    "        -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pred_masks: [B, nb_preds, nb_per_preds, H, W]\n",
    "        gt_mask: [B, H, W]\n",
    "    \"\"\"\n",
    "    low_res_masks = pred_masks\n",
    "    upscaled_masks = postprocess_masks(\n",
    "        low_res_masks.squeeze(1),\n",
    "        batch[\"reshaped_input_sizes\"][0].tolist(),\n",
    "        batch[\"original_sizes\"][0].tolist())\n",
    "    mask_prob = torch.sigmoid(upscaled_masks)\n",
    "    # convert soft mask to hard mask\n",
    "    mask_prob = mask_prob.cpu().detach().squeeze(1)\n",
    "    sam_mask = (mask_prob > 0.5).to(torch.uint8)\n",
    "    iou = mask_iou(sam_mask.numpy(), gt_mask.numpy())\n",
    "    dsc = DSC(sam_mask.numpy(), gt_mask.numpy())\n",
    "    return iou, dsc\n",
    "\n",
    "\n",
    "def mkdir_if_not_exists(path, delete=False):\n",
    "    if os.path.exists(path):\n",
    "        if delete:\n",
    "            shutil.rmtree(path)\n",
    "            os.makedirs(path)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "def save_model(model, optimizer, epoch,\n",
    "               save_path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, save_path)\n",
    "\n",
    "\n",
    "def resume(model, optimizer, resume_path, device):\n",
    "    if not os.path.exists(resume_path):\n",
    "        print(f'Cannot find {resume_path}')\n",
    "        return model, optimizer, 0\n",
    "\n",
    "    ckp = torch.load(resume_path, map_location=device)\n",
    "    model_dict = ckp['model_state_dict']\n",
    "    opt_dict = ckp['optimizer_state_dict']\n",
    "    epoch = ckp['epoch']\n",
    "\n",
    "    model.load_state_dict(model_dict)\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(opt_dict)\n",
    "    else:\n",
    "        print('Optimizer is None')\n",
    "        optimizer = opt_dict\n",
    "\n",
    "    print('Resume from Path {} '.format(resume_path))\n",
    "    print('Resume from epoch {} '.format(epoch)\n",
    "          + '*' * 100)\n",
    "    return model, optimizer, epoch\n",
    "\n",
    "\n",
    "class PSVDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 root='/home/jiashuo/workspace/datasets/parking_slots/PSV dataset/',\n",
    "                 split='test',\n",
    "                 ds_size=1.) -> None:\n",
    "        super().__init__()\n",
    "        label_path = os.path.join(root, f'{split}.txt')\n",
    "        with open(label_path, 'r') as f:\n",
    "            samples = f.readlines()\n",
    "        self.image_root = os.path.join(root, 'images', split)\n",
    "        self.label_root = os.path.join(root, 'labels', split)\n",
    "        self.samples = [sample.strip() for sample in samples]\n",
    "        if ds_size < 0.99:\n",
    "            ds_len = int(len(self.samples) * ds_size)\n",
    "            random.shuffle(self.samples)\n",
    "            self.samples = self.samples[:ds_len]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, np.ndarray]:\n",
    "        sample_id = self.samples[index]\n",
    "        image = cv2.imread(os.path.join(self.image_root, f'{sample_id}.jpg'))\n",
    "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "        mask = self.get_mask(sample_id)\n",
    "        # resize\n",
    "        rgb_image = cv2.resize(rgb_image, (600, 600),\n",
    "                               interpolation=cv2.INTER_LINEAR)\n",
    "        lab_image = cv2.resize(lab_image, (600, 600),\n",
    "                               interpolation=cv2.INTER_LINEAR)\n",
    "        mask = cv2.resize(mask, (600, 600), interpolation=cv2.INTER_LINEAR)\n",
    "        return {'image': image, 'label': mask, 'lab_image': lab_image}\n",
    "\n",
    "    def get_mask(self, sample_id) -> np.ndarray:\n",
    "        mask_path: str = os.path.join(self.label_root, f'{sample_id}.png')\n",
    "        mask = np.array(Image.open(mask_path)).astype(np.uint8)\n",
    "        # eliminate class impact\n",
    "        mask = np.array(np.bool_(mask), dtype=np.uint8)\n",
    "        assert len(np.unique(mask)) <= 2, \\\n",
    "            f'Invalid mask value: {np.unique(mask)}'\n",
    "        return mask\n",
    "\n",
    "\n",
    "class SAMDataset(Dataset):\n",
    "    def __init__(self, dataset, processor, noise_rate: int = 0):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.noise_rate = noise_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        item = self.dataset[idx]\n",
    "        image = item[\"image\"]\n",
    "        ground_truth_mask = np.array(item[\"label\"])\n",
    "        if self.noise_rate > 0:\n",
    "            ground_truth_mask = dilate_mask(ground_truth_mask, 2,\n",
    "                                            self.noise_rate)\n",
    "\n",
    "        # get prompt by erosion\n",
    "        prompt = get_point_prompt_bymask(ground_truth_mask)\n",
    "\n",
    "        # prepare image and prompt for the model\n",
    "        inputs = self.processor(image, **prompt, return_tensors=\"pt\")\n",
    "\n",
    "        # remove batch dimension which the processor adds by default\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "\n",
    "        # add lab tensor image\n",
    "        inputs['lab_image'] = torch.as_tensor(item['lab_image']).permute(2, 0, 1)\n",
    "        # add ground truth segmentation\n",
    "        inputs[\"ground_truth_mask\"] = ground_truth_mask\n",
    "\n",
    "        return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = SamModel.from_pretrained(\"facebook/sam-vit-huge\", mirror='tuna').to(device)\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\", mirror='tuna')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "psv_set = PSVDataset(split='train')\n",
    "train_dataset = SAMDataset(dataset=psv_set,\n",
    "                           processor=processor)\n",
    "\n",
    "# for i in range(len(train_dataset)):\n",
    "#     example = train_dataset[i]\n",
    "#     # for k, v in example.items():\n",
    "#     #     print(k, v.shape)\n",
    "# print(wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1000, 1000],\n",
      "        [ 600,  600]])\n",
      "tensor([1000, 1000])\n",
      "tensor([[1024, 1024],\n",
      "        [1024, 1024]])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "# for batch in tqdm(train_dataloader):\n",
    "#   pass\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch['original_sizes'])\n",
    "print(batch['original_sizes'][0])\n",
    "print(batch['reshaped_input_sizes'])\n",
    "\n",
    "# t = 0\n",
    "# for b in train_dataloader:\n",
    "#     t += 1\n",
    "#     if t % 100 == 0:\n",
    "#         print(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                input_points=batch[\"input_points\"].to(device),\n",
    "                input_labels =batch[\"input_labels\"].to(device),\n",
    "                multimask_output=False)\n",
    "print(outputs.keys())\n",
    "print(outputs.pred_masks)\n",
    "print(batch[\"ground_truth_mask\"].float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from transformers.models.maskformer.modeling_maskformer import dice_loss, sigmoid_focal_loss\n",
    "\n",
    "\n",
    "def postprocess_masks(masks: torch.Tensor,\n",
    "                      input_size: Tuple[int, ...],\n",
    "                      original_size: Tuple[int, ...], image_size=1024) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Remove padding and upscale masks to the original image size.\n",
    "\n",
    "    Args:\n",
    "      masks (torch.Tensor):\n",
    "        Batched masks from the mask_decoder, in BxCxHxW format.\n",
    "      input_size (tuple(int, int)):\n",
    "        The size of the image input to the model, in (H', W') format. Used to remove padding.\n",
    "      original_size (tuple(int, int)):\n",
    "        The original size of the image before resizing for input to the model, in (H, W) format.\n",
    "\n",
    "    Returns:\n",
    "      (torch.Tensor): Batched masks in BxCxHxW format, where (H, W)\n",
    "        is given by original_size.\n",
    "    \"\"\"\n",
    "    masks = F.interpolate(\n",
    "        masks,\n",
    "        (image_size, image_size),\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "    masks = masks[..., : input_size[0], : input_size[1]]\n",
    "    masks = F.interpolate(masks, original_size,\n",
    "                          mode=\"bilinear\", align_corners=False)\n",
    "    return masks\n",
    "\n",
    "print(outputs.keys())\n",
    "# [bt_size, nb_predictions, nb_per_pred, H, W]\n",
    "# [bt_size, 1, H, W]\n",
    "low_res_masks = outputs.pred_masks\n",
    "upscaled_masks = postprocess_masks(\n",
    "    low_res_masks.squeeze(1),\n",
    "    batch[\"reshaped_input_sizes\"][0].tolist(),\n",
    "    batch[\"original_sizes\"][0].tolist()).to(device)\n",
    "\n",
    "'process upscaled masks'\n",
    "# compute iou by thresholding\n",
    "predicted_masks = torch.sigmoid(upscaled_masks)\n",
    "# predicted_masks = \\\n",
    "#             threshold(upscaled_masks, 0.0, 0)\n",
    "# predicted_masks = normalize(\n",
    "#             threshold(upscaled_masks, 0.0, 0))\n",
    "print(predicted_masks.shape)\n",
    "gt_mask = batch[\"ground_truth_mask\"].to(device)\n",
    "print('mask.shape:    ', predicted_masks.shape)\n",
    "print('gt_mask.shape: ', gt_mask.shape)\n",
    "batch_tp, batch_fp, batch_fn, batch_tn = smp.metrics.get_stats(\n",
    "    predicted_masks,\n",
    "    gt_mask.unsqueeze(1),\n",
    "    mode='binary',\n",
    "    threshold=0.5,\n",
    ")\n",
    "batch_iou = smp.metrics.iou_score(batch_tp, batch_fp, batch_fn, batch_tn)\n",
    "print('iou_scores.shape: ', outputs.iou_scores.shape)\n",
    "print('batch_iou.shape:  ', batch_iou.shape)\n",
    "loss_iou = F.mse_loss(outputs.iou_scores.squeeze(1), \n",
    "                      batch_iou, reduction='mean')\n",
    "print('batch_tp : ',  batch_tp.data)\n",
    "print('batch_fp : ',  batch_fp.data)\n",
    "print('batch_fn : ',  batch_fn.data)\n",
    "print('batch_tn : ',  batch_tn.data)\n",
    "print('batch_iou: ', batch_iou.data)\n",
    "print('loss_iou : ', loss_iou.data)\n",
    "\n",
    "# compute focal and dice loss\n",
    "mask_logits = upscaled_masks.flatten(1)\n",
    "gt_mask_logits = gt_mask.flatten(1)\n",
    "nb_masks = mask_logits.shape[0]\n",
    "print('mask_logits.shape   : ', mask_logits.shape)\n",
    "print('gt_mask_logits.shape: ', gt_mask_logits.shape)\n",
    "loss_focal = sigmoid_focal_loss(mask_logits, gt_mask_logits.float(), nb_masks)\n",
    "loss_dice = dice_loss(mask_logits, gt_mask_logits.float(), nb_masks)\n",
    "print('loss_focal: ', loss_focal.data)\n",
    "print('loss_dice : ', loss_dice.data)\n",
    "\n",
    "def criterion_mse(outputs, gt_mask, batch):\n",
    "    low_res_masks = outputs.pred_masks\n",
    "    upscaled_masks = postprocess_masks(\n",
    "        low_res_masks.squeeze(1),\n",
    "        batch[\"reshaped_input_sizes\"][0].tolist(),\n",
    "        batch[\"original_sizes\"][0].tolist()).to(gt_mask.device)\n",
    "    predicted_masks = normalize(threshold(upscaled_masks, 0.0, 0))\n",
    "    loss = torch.nn.MSELoss(reduction='mean')(predicted_masks, gt_mask.unsqueeze(1))\n",
    "    print(predicted_masks.shape, gt_mask.shape, loss.data)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def criterion_mde(outputs, gt_mask, batch):\n",
    "    low_res_masks = outputs.pred_masks\n",
    "    upscaled_masks = postprocess_masks(\n",
    "        low_res_masks.squeeze(1),\n",
    "        batch[\"reshaped_input_sizes\"][0].tolist(),\n",
    "        batch[\"original_sizes\"][0].tolist()).to(gt_mask.device)\n",
    "    seg_loss = monai.losses.DiceCELoss(sigmoid=True,\n",
    "                                       squared_pred=True,\n",
    "                                       reduction='mean')\n",
    "    loss = seg_loss(upscaled_masks,\n",
    "                    gt_mask.unsqueeze(1))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def criterion_sam(outputs, gt_mask, batch):\n",
    "    low_res_masks = outputs.pred_masks\n",
    "    upscaled_masks = postprocess_masks(\n",
    "        low_res_masks.squeeze(1),\n",
    "        batch[\"reshaped_input_sizes\"][0].tolist(),\n",
    "        batch[\"original_sizes\"][0].tolist()).to(gt_mask.device)\n",
    "    'process upscaled masks'\n",
    "    '''Compute iou by thresholding\n",
    "    predicted_masks = \\\n",
    "                threshold(upscaled_masks, 0.0, 0)\n",
    "    predicted_masks = normalize(\n",
    "                threshold(upscaled_masks, 0.0, 0))\n",
    "    '''\n",
    "    predicted_masks = torch.sigmoid(upscaled_masks)\n",
    "    batch_tp, batch_fp, batch_fn, batch_tn = smp.metrics.get_stats(\n",
    "        predicted_masks,\n",
    "        gt_mask.unsqueeze(1),\n",
    "        mode='binary',\n",
    "        threshold=0.5,\n",
    "    )\n",
    "    batch_iou = smp.metrics.iou_score(batch_tp, batch_fp, batch_fn, batch_tn)\n",
    "    loss_iou = F.mse_loss(outputs.iou_scores.squeeze(1), \n",
    "                        batch_iou, reduction='mean')\n",
    "    # compute focal and dice loss\n",
    "    mask_logits = upscaled_masks.flatten(1)\n",
    "    gt_mask_logits = gt_mask.flatten(1).float()\n",
    "    nb_masks = mask_logits.shape[0]\n",
    "    loss_focal = sigmoid_focal_loss(mask_logits, gt_mask_logits, nb_masks)\n",
    "    loss_dice = dice_loss(mask_logits, gt_mask_logits, nb_masks)\n",
    "    return loss_iou + loss_focal * 20. + loss_dice\n",
    "\n",
    "print(criterion_sam(outputs, gt_mask, batch))\n",
    "print(criterion_mde(outputs, gt_mask, batch))\n",
    "print(criterion_mse(outputs, gt_mask, batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_res_masks = outputs.pred_masks\n",
    "upscaled_masks = postprocess_masks(\n",
    "    low_res_masks.squeeze(1),\n",
    "    batch[\"reshaped_input_sizes\"][0].tolist(),\n",
    "    batch[\"original_sizes\"][0].tolist())\n",
    "mask_prob = torch.sigmoid(upscaled_masks)\n",
    "# convert soft mask to hard mask\n",
    "mask_prob = mask_prob.cpu().detach().squeeze(1)\n",
    "sam_mask = (mask_prob > 0.5).to(torch.uint8)\n",
    "mask_prob.shape\n",
    "# iou = mask_iou(sam_mask.numpy(), gt_mask.numpy())\n",
    "# dsc = DSC(sam_mask.numpy(), gt_mask.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QAQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
