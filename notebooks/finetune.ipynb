{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import monai\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import threshold, normalize\n",
    "\n",
    "from transformers import SamProcessor\n",
    "from transformers import SamModel\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "def get_disturbed_bounding_box(ground_truth_map: np.ndarray) -> List:\n",
    "    # get bounding box from mask\n",
    "    y_indices, x_indices = np.where(ground_truth_map > 0)\n",
    "    x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "    y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "    # add perturbation to bounding box coordinates\n",
    "    H, W = ground_truth_map.shape\n",
    "    x_min = max(0, x_min - np.random.randint(0, 20))\n",
    "    x_max = min(W, x_max + np.random.randint(0, 20))\n",
    "    y_min = max(0, y_min - np.random.randint(0, 20))\n",
    "    y_max = min(H, y_max + np.random.randint(0, 20))\n",
    "    bbox = [x_min, y_min, x_max, y_max]\n",
    "    return bbox\n",
    "\n",
    "\n",
    "def erode_mask(mask: np.ndarray, kernel_size: int = 3) -> np.ndarray:\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "    mask = cv2.erode(mask, kernel, iterations=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_point_prompt_bymask(mask: np.ndarray) -> np.ndarray:\n",
    "    nb = 100\n",
    "    mask = erode_mask(mask)\n",
    "    bg_points = np.where(mask == 0)\n",
    "    bg_points = [list(i) for i in bg_points]\n",
    "    bg_points = list(zip(bg_points[1], bg_points[0]))\n",
    "    assert len(bg_points) > 0\n",
    "    indices = np.random.choice(\n",
    "        np.arange(len(bg_points)),\n",
    "        size=nb, replace=False) \\\n",
    "        if len(bg_points) >= nb else \\\n",
    "        np.random.choice(\n",
    "            np.arange(len(bg_points)),\n",
    "            size=nb, replace=True)\n",
    "    bg_points = np.array(bg_points)[indices]\n",
    "    bg_label = np.zeros(nb,)\n",
    "\n",
    "    fg_points = np.where(mask != 0)\n",
    "    fg_points = [list(i) for i in fg_points]\n",
    "    fg_points = list(zip(fg_points[1], fg_points[0]))\n",
    "    if len(fg_points) > 0:\n",
    "        indices = np.random.choice(\n",
    "            np.arange(len(fg_points)),\n",
    "            size=nb, replace=False) \\\n",
    "            if len(fg_points) >= nb else \\\n",
    "            np.random.choice(\n",
    "                np.arange(len(fg_points)),\n",
    "                size=nb, replace=True)\n",
    "        fg_points = np.array(fg_points)[indices]\n",
    "        fg_label = np.ones(nb,)\n",
    "        pmt_points = [[np.vstack([fg_points, bg_points]).tolist()]]\n",
    "        pmt_labels = [[np.hstack([fg_label, bg_label]).tolist()]]\n",
    "    else:\n",
    "        fg_points = []\n",
    "        fg_label = []\n",
    "        pmt_points = [[np.vstack([bg_points, bg_points]).tolist()]]\n",
    "        pmt_labels = [[np.hstack([bg_label, bg_label]).tolist()]]\n",
    "    # pmt_points = np.vstack([fg_points, bg_points])\n",
    "    # pmt_labels = np.hstack([fg_label, bg_label])\n",
    "    return {'input_points': pmt_points,\n",
    "            'input_labels': pmt_labels}\n",
    "\n",
    "class PSVDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 root='/home/jiashuo/workspace/datasets/parking_slots/PSV dataset/',\n",
    "                 split='test') -> None:\n",
    "        super().__init__()\n",
    "        label_path = os.path.join(root, f'{split}.txt')\n",
    "        with open(label_path, 'r') as f:\n",
    "            samples = f.readlines()\n",
    "        self.samples = [sample.strip() for sample in samples]\n",
    "        self.image_root = os.path.join(root, 'images', split)\n",
    "        self.label_root = os.path.join(root, 'labels', split)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, np.ndarray]:\n",
    "        sample_id = self.samples[index]\n",
    "        image = cv2.imread(os.path.join(self.image_root, f'{sample_id}.jpg'))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = self.get_mask(sample_id)\n",
    "        # resize\n",
    "        image = cv2.resize(image, (256, 256), interpolation=cv2.INTER_LINEAR)\n",
    "        mask = cv2.resize(mask, (256, 256), interpolation=cv2.INTER_LINEAR)\n",
    "        return {'image': image, 'label': mask}\n",
    "\n",
    "    def get_mask(self, sample_id):\n",
    "        mask_path = os.path.join(self.label_root, f'{sample_id}.png')\n",
    "        mask = np.array(Image.open(mask_path)).astype(np.uint8)\n",
    "        # eliminate class impact\n",
    "        mask = np.bool_(mask).astype(np.uint8)\n",
    "        return mask\n",
    "\n",
    "\n",
    "class SAMDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item[\"image\"]\n",
    "        ground_truth_mask = np.array(item[\"label\"])\n",
    "\n",
    "        # get prompt by erosion\n",
    "        prompt = get_point_prompt_bymask(ground_truth_mask)\n",
    "\n",
    "        # prepare image and prompt for the model\n",
    "        inputs = self.processor(image, **prompt, return_tensors=\"pt\")\n",
    "\n",
    "        # remove batch dimension which the processor adds by default\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "\n",
    "        # add ground truth segmentation\n",
    "        inputs[\"ground_truth_mask\"] = ground_truth_mask\n",
    "\n",
    "        return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-huge\", mirror='tuna').to(device)\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\", mirror='tuna')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psv_set = PSVDataset(split='train')\n",
    "train_dataset = SAMDataset(dataset=psv_set,\n",
    "                           processor=processor)\n",
    "\n",
    "# for i in range(len(train_dataset)):\n",
    "#     example = train_dataset[i]\n",
    "#     # for k, v in example.items():\n",
    "#     #     print(k, v.shape)\n",
    "# print(wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "# for batch in tqdm(train_dataloader):\n",
    "#   pass\n",
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "  print(k,v.shape)\n",
    "outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                input_points=batch[\"input_points\"].to(device),\n",
    "                input_labels =batch[\"input_labels\"].to(device),\n",
    "                multimask_output=False)\n",
    "print(outputs.keys())\n",
    "print(outputs.pred_masks)\n",
    "print(batch[\"ground_truth_mask\"].float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from transformers.models.maskformer.modeling_maskformer import dice_loss, sigmoid_focal_loss\n",
    "\n",
    "\n",
    "def postprocess_masks(masks: torch.Tensor,\n",
    "                      input_size: Tuple[int, ...],\n",
    "                      original_size: Tuple[int, ...], image_size=1024) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Remove padding and upscale masks to the original image size.\n",
    "\n",
    "    Args:\n",
    "      masks (torch.Tensor):\n",
    "        Batched masks from the mask_decoder, in BxCxHxW format.\n",
    "      input_size (tuple(int, int)):\n",
    "        The size of the image input to the model, in (H', W') format. Used to remove padding.\n",
    "      original_size (tuple(int, int)):\n",
    "        The original size of the image before resizing for input to the model, in (H, W) format.\n",
    "\n",
    "    Returns:\n",
    "      (torch.Tensor): Batched masks in BxCxHxW format, where (H, W)\n",
    "        is given by original_size.\n",
    "    \"\"\"\n",
    "    masks = F.interpolate(\n",
    "        masks,\n",
    "        (image_size, image_size),\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "    masks = masks[..., : input_size[0], : input_size[1]]\n",
    "    masks = F.interpolate(masks, original_size,\n",
    "                          mode=\"bilinear\", align_corners=False)\n",
    "    return masks\n",
    "\n",
    "print(outputs.keys())\n",
    "# [bt_size, nb_predictions, nb_per_pred, H, W]\n",
    "# [bt_size, 1, H, W]\n",
    "low_res_masks = outputs.pred_masks\n",
    "upscaled_masks = postprocess_masks(\n",
    "    low_res_masks.squeeze(1),\n",
    "    batch[\"reshaped_input_sizes\"][0].tolist(),\n",
    "    batch[\"original_sizes\"][0].tolist()).to(device)\n",
    "\n",
    "'process upscaled masks'\n",
    "# compute iou by thresholding\n",
    "predicted_masks = torch.sigmoid(upscaled_masks)\n",
    "# predicted_masks = \\\n",
    "#             threshold(upscaled_masks, 0.0, 0)\n",
    "# predicted_masks = normalize(\n",
    "#             threshold(upscaled_masks, 0.0, 0))\n",
    "print(predicted_masks.shape)\n",
    "gt_mask = batch[\"ground_truth_mask\"].to(device)\n",
    "print('mask.shape:    ', predicted_masks.shape)\n",
    "print('gt_mask.shape: ', gt_mask.shape)\n",
    "batch_tp, batch_fp, batch_fn, batch_tn = smp.metrics.get_stats(\n",
    "    predicted_masks,\n",
    "    gt_mask.unsqueeze(1),\n",
    "    mode='binary',\n",
    "    threshold=0.5,\n",
    ")\n",
    "batch_iou = smp.metrics.iou_score(batch_tp, batch_fp, batch_fn, batch_tn)\n",
    "print('iou_scores.shape: ', outputs.iou_scores.shape)\n",
    "print('batch_iou.shape:  ', batch_iou.shape)\n",
    "loss_iou = F.mse_loss(outputs.iou_scores.squeeze(1), \n",
    "                      batch_iou, reduction='mean')\n",
    "print('batch_tp : ',  batch_tp.data)\n",
    "print('batch_fp : ',  batch_fp.data)\n",
    "print('batch_fn : ',  batch_fn.data)\n",
    "print('batch_tn : ',  batch_tn.data)\n",
    "print('batch_iou: ', batch_iou.data)\n",
    "print('loss_iou : ', loss_iou.data)\n",
    "\n",
    "# compute focal and dice loss\n",
    "mask_logits = upscaled_masks.flatten(1)\n",
    "gt_mask_logits = gt_mask.flatten(1)\n",
    "nb_masks = mask_logits.shape[0]\n",
    "print('mask_logits.shape   : ', mask_logits.shape)\n",
    "print('gt_mask_logits.shape: ', gt_mask_logits.shape)\n",
    "loss_focal = sigmoid_focal_loss(mask_logits, gt_mask_logits.float(), nb_masks)\n",
    "loss_dice = dice_loss(mask_logits, gt_mask_logits.float(), nb_masks)\n",
    "print('loss_focal: ', loss_focal.data)\n",
    "print('loss_dice : ', loss_dice.data)\n",
    "\n",
    "def criterion_mse(outputs, gt_mask, batch):\n",
    "    low_res_masks = outputs.pred_masks\n",
    "    upscaled_masks = postprocess_masks(\n",
    "        low_res_masks.squeeze(1),\n",
    "        batch[\"reshaped_input_sizes\"][0].tolist(),\n",
    "        batch[\"original_sizes\"][0].tolist()).to(gt_mask.device)\n",
    "    predicted_masks = normalize(threshold(upscaled_masks, 0.0, 0))\n",
    "    loss = torch.nn.MSELoss(reduction='mean')(predicted_masks, gt_mask.unsqueeze(1))\n",
    "    print(predicted_masks.shape, gt_mask.shape, loss.data)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def criterion_mde(outputs, gt_mask, batch):\n",
    "    low_res_masks = outputs.pred_masks\n",
    "    upscaled_masks = postprocess_masks(\n",
    "        low_res_masks.squeeze(1),\n",
    "        batch[\"reshaped_input_sizes\"][0].tolist(),\n",
    "        batch[\"original_sizes\"][0].tolist()).to(gt_mask.device)\n",
    "    seg_loss = monai.losses.DiceCELoss(sigmoid=True,\n",
    "                                       squared_pred=True,\n",
    "                                       reduction='mean')\n",
    "    loss = seg_loss(upscaled_masks,\n",
    "                    gt_mask.unsqueeze(1))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def criterion_sam(outputs, gt_mask, batch):\n",
    "    low_res_masks = outputs.pred_masks\n",
    "    upscaled_masks = postprocess_masks(\n",
    "        low_res_masks.squeeze(1),\n",
    "        batch[\"reshaped_input_sizes\"][0].tolist(),\n",
    "        batch[\"original_sizes\"][0].tolist()).to(gt_mask.device)\n",
    "    'process upscaled masks'\n",
    "    '''Compute iou by thresholding\n",
    "    predicted_masks = \\\n",
    "                threshold(upscaled_masks, 0.0, 0)\n",
    "    predicted_masks = normalize(\n",
    "                threshold(upscaled_masks, 0.0, 0))\n",
    "    '''\n",
    "    predicted_masks = torch.sigmoid(upscaled_masks)\n",
    "    batch_tp, batch_fp, batch_fn, batch_tn = smp.metrics.get_stats(\n",
    "        predicted_masks,\n",
    "        gt_mask.unsqueeze(1),\n",
    "        mode='binary',\n",
    "        threshold=0.5,\n",
    "    )\n",
    "    batch_iou = smp.metrics.iou_score(batch_tp, batch_fp, batch_fn, batch_tn)\n",
    "    loss_iou = F.mse_loss(outputs.iou_scores.squeeze(1), \n",
    "                        batch_iou, reduction='mean')\n",
    "    # compute focal and dice loss\n",
    "    mask_logits = upscaled_masks.flatten(1)\n",
    "    gt_mask_logits = gt_mask.flatten(1).float()\n",
    "    nb_masks = mask_logits.shape[0]\n",
    "    loss_focal = sigmoid_focal_loss(mask_logits, gt_mask_logits, nb_masks)\n",
    "    loss_dice = dice_loss(mask_logits, gt_mask_logits, nb_masks)\n",
    "    return loss_iou + loss_focal * 20. + loss_dice\n",
    "\n",
    "print(criterion_sam(outputs, gt_mask, batch))\n",
    "print(criterion_mde(outputs, gt_mask, batch))\n",
    "print(criterion_mse(outputs, gt_mask, batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_res_masks = outputs.pred_masks\n",
    "upscaled_masks = postprocess_masks(\n",
    "    low_res_masks.squeeze(1),\n",
    "    batch[\"reshaped_input_sizes\"][0].tolist(),\n",
    "    batch[\"original_sizes\"][0].tolist())\n",
    "mask_prob = torch.sigmoid(upscaled_masks)\n",
    "# convert soft mask to hard mask\n",
    "mask_prob = mask_prob.cpu().detach().squeeze(1)\n",
    "sam_mask = (mask_prob > 0.5).to(torch.uint8)\n",
    "mask_prob.shape\n",
    "# iou = mask_iou(sam_mask.numpy(), gt_mask.numpy())\n",
    "# dsc = DSC(sam_mask.numpy(), gt_mask.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QAQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
